{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ScrapBook.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "qdualDrQYEB1"
      ],
      "mount_file_id": "1OdKdzS8JW90Pn4rwX9su50UPPmfn4rk9",
      "authorship_tag": "ABX9TyPpY51rVtN5La2NpwrfJTFD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ocampos16/AdvancedProject1/blob/master/Environmental%20Policies%20Hashtag%20Team/SentimentAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDEDTGoS-nEF",
        "colab_type": "text"
      },
      "source": [
        "## download libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mp_GqazZ9Jj9",
        "colab_type": "code",
        "outputId": "a81dfbf2-88ea-4c60-b149-fae88b7913bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        }
      },
      "source": [
        "!pip install emoji --upgrade\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('sentiwordnet')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting emoji\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/8d/521be7f0091fe0f2ae690cc044faf43e3445e0ff33c574eae752dd7e39fa/emoji-0.5.4.tar.gz (43kB)\n",
            "\r\u001b[K     |███████▌                        | 10kB 17.3MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 30kB 2.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 40kB 2.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 1.7MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-0.5.4-cp36-none-any.whl size=42176 sha256=e4f1f04666d924876fa4a2c6e2584c6d2d6ae69718f40e7c2988fda941f38eb5\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/a9/0a/4f8e8cce8074232aba240caca3fade315bb49fac68808d1a9c\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-0.5.4\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package sentiwordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lyA7gI_Xqr4",
        "colab_type": "text"
      },
      "source": [
        "## Import necessary libraries for data cleaning, sentiment analysis and word embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SENp0Deh6tiu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import json\n",
        "import emoji\n",
        "\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from nltk import pos_tag, sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords, wordnet as wn, sentiwordnet as swn\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdualDrQYEB1",
        "colab_type": "text"
      },
      "source": [
        "## Define dictionary for contractions of words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAN6Ouc-7UGO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dictionary of contarctions. source for contractions: https://en.wikipedia.org/wiki/Contraction_%28grammar%29\n",
        "contractions_dict = {\n",
        "        \"ain't\":\"is not\",\n",
        "        \"amn't\":\"am not\",\n",
        "        \"aren't\":\"are not\",\n",
        "        \"can't\":\"cannot\",\n",
        "        \"'cause\":\"because\",\n",
        "        \"couldn't\":\"could not\",\n",
        "        \"couldn't've\":\"could not have\",\n",
        "        \"could've\":\"could have\",\n",
        "        \"daren't\":\"dare not\",\n",
        "        \"daresn't\":\"dare not\",\n",
        "        \"dasn't\":\"dare not\",\n",
        "        \"didn't\":\"did not\",\n",
        "        \"doesn't\":\"does not\",\n",
        "        \"don't\":\"do not\",\n",
        "        \"e'er\":\"ever\",\n",
        "        \"em\":\"them\",\n",
        "        \"everyone's\":\"everyone is\",\n",
        "        \"finna\":\"fixing to\",\n",
        "        \"gimme\":\"give me\",\n",
        "        \"gonna\":\"going to\",\n",
        "        \"gon't\":\"go not\",\n",
        "        \"gotta\":\"got to\",\n",
        "        \"hadn't\":\"had not\",\n",
        "        \"hasn't\":\"has not\",\n",
        "        \"haven't\":\"have not\",\n",
        "        \"he'd\":\"he would\",\n",
        "        \"he'll\":\"he will\",\n",
        "        \"he's\":\"he is\",\n",
        "        \"he've\":\"he have\",\n",
        "        \"how'd\":\"how would\",\n",
        "        \"how'll\":\"how will\",\n",
        "        \"how're\":\"how are\",\n",
        "        \"how's\":\"how is\",\n",
        "        \"I'd\":\"I would\",\n",
        "        \"I'll\":\"I will\",\n",
        "        \"I'm\":\"I am\",\n",
        "        \"I'm'a\":\"I am about to\",\n",
        "        \"I'm'o\":\"I am going to\",\n",
        "        \"isn't\":\"is not\",\n",
        "        \"it'd\":\"it would\",\n",
        "        \"it'll\":\"it will\",\n",
        "        \"it's\":\"it is\",\n",
        "        \"I've\":\"I have\",\n",
        "        \"kinda\":\"kind of\",\n",
        "        \"let's\":\"let us\",\n",
        "        \"mayn't\":\"may not\",\n",
        "        \"may've\":\"may have\",\n",
        "        \"mightn't\":\"might not\",\n",
        "        \"might've\":\"might have\",\n",
        "        \"mustn't\":\"must not\",\n",
        "        \"mustn't've\":\"must not have\",\n",
        "        \"must've\":\"must have\",\n",
        "        \"needn't\":\"need not\",\n",
        "        \"ne'er\":\"never\",\n",
        "        \"o'\":\"of\",\n",
        "        \"o'er\":\"over\",\n",
        "        \"ol'\":\"old\",\n",
        "        \"oughtn't\":\"ought not\",\n",
        "        \"shalln't\":\"shall not\",\n",
        "        \"shan't\":\"shall not\",\n",
        "        \"she'd\":\"she would\",\n",
        "        \"she'll\":\"she will\",\n",
        "        \"she's\":\"she is\",\n",
        "        \"shouldn't\":\"should not\",\n",
        "        \"shouldn't've\":\"should not have\",\n",
        "        \"should've\":\"should have\",\n",
        "        \"somebody's\":\"somebody is\",\n",
        "        \"someone's\":\"someone is\",\n",
        "        \"something's\":\"something is\",\n",
        "        \"that'd\":\"that would\",\n",
        "        \"that'll\":\"that will\",\n",
        "        \"that're\":\"that are\",\n",
        "        \"that's\":\"that is\",\n",
        "        \"there'd\":\"there would\",\n",
        "        \"there'll\":\"there will\",\n",
        "        \"there're\":\"there are\",\n",
        "        \"there's\":\"there is\",\n",
        "        \"these're\":\"these are\",\n",
        "        \"they'd\":\"they would\",\n",
        "        \"they'll\":\"they will\",\n",
        "        \"they're\":\"they are\",\n",
        "        \"they've\":\"they have\",\n",
        "        \"this's\":\"this is\",\n",
        "        \"those're\":\"those are\",\n",
        "        \"'tis\":\"it is\",\n",
        "        \"'twas\":\"it was\",\n",
        "        \"wanna\":\"want to\",\n",
        "        \"wasn't\":\"was not\",\n",
        "        \"we'd\":\"we would\",\n",
        "        \"we'd've\":\"we would have\",\n",
        "        \"we'll\":\"we will\",\n",
        "        \"we're\":\"we are\",\n",
        "        \"weren't\":\"were not\",\n",
        "        \"we've\":\"we have\",\n",
        "        \"what'd\":\"what did\",\n",
        "        \"what'll\":\"what will\",\n",
        "        \"what're\":\"what are\",\n",
        "        \"what's\":\"what is\",\n",
        "        \"what've\":\"what have\",\n",
        "        \"when's\":\"when is\",\n",
        "        \"where'd\":\"where did\",\n",
        "        \"where're\":\"where are\",\n",
        "        \"where's\":\"where is\",\n",
        "        \"where've\":\"where have\",\n",
        "        \"which's\":\"which is\",\n",
        "        \"who'd\":\"who would\",\n",
        "        \"who'd've\":\"who would have\",\n",
        "        \"who'll\":\"who will\",\n",
        "        \"who're\":\"who are\",\n",
        "        \"who's\":\"who is\",\n",
        "        \"who've\":\"who have\",\n",
        "        \"why'd\":\"why did\",\n",
        "        \"why're\":\"why are\",\n",
        "        \"why's\":\"why is\",\n",
        "        \"won't\":\"will not\",\n",
        "        \"wouldn't\":\"would not\",\n",
        "        \"would've\":\"would have\",\n",
        "        \"y'all\":\"you all\",\n",
        "        \"you'd\":\"you would\",\n",
        "        \"you'll\":\"you will\",\n",
        "        \"you're\":\"you are\",\n",
        "        \"you've\":\"you have\"\n",
        "        }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_El7jaNYXcb",
        "colab_type": "text"
      },
      "source": [
        "##Read json formatted raw data as a DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPOGU3Cb6jjd",
        "colab_type": "code",
        "outputId": "e2aa5c12-17f5-45ea-ed28-a075aed1774f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "# Input data files are available in the \"dataset/\" directory.\n",
        "# Read the input raw data and parse only the necessary columns from the json file.\n",
        "\n",
        "def populate_tweet_df(tweets):\n",
        "    \"\"\"\n",
        "    This function takes tweets list as argument and returns a dataframe of the tweets.\n",
        "    \"\"\"\n",
        "    df = pd.DataFrame()\n",
        "    df['date'] = [datetime.strptime(tweet['created_at'],'%a %b %d %H:%M:%S %z %Y') for tweet in tweets if (tweet['lang'] == 'en')]\n",
        "    df['text'] = [tweet['text'] for tweet in tweets if (tweet['lang'] == 'en')]\n",
        "    # df['text'] = list(map(lambda tweet:tweet['text'], filter(lambda tweet: tweet['lang']=='en', tweets)))\n",
        "    return df\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    tweet_file = (\"/content/drive/My Drive/dataScience/projects/twitterSentimentAnalysis/dataset/tweets_parisagreement_09-03-2020.txt\")\n",
        "    tweets = []\n",
        "    with open(tweet_file, 'r') as file:\n",
        "        for line in file.readlines():\n",
        "            tweets.append(json.loads(line))\n",
        "    tweets_df = populate_tweet_df(tweets)\n",
        "    print(\"Executed Successfully\")\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Executed Successfully\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpSJiYfeYoVB",
        "colab_type": "code",
        "outputId": "9704559b-bdb8-48aa-ca55-f16abb44b905",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "tweets_df.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2020-03-09 13:49:46+00:00</td>\n",
              "      <td>One need World to make #MAGA .\\nWe need #Paris...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2020-03-09 13:26:18+00:00</td>\n",
              "      <td>SLOW DOWN\\nGlobal #CO2 emissions from the #pow...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2020-03-09 13:21:51+00:00</td>\n",
              "      <td>RT @UNDPClimate: This Wednesday 11 March Switz...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2020-03-09 13:21:42+00:00</td>\n",
              "      <td>RT @UNDPClimate: This Wednesday 11 March Switz...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2020-03-09 13:20:13+00:00</td>\n",
              "      <td>This Wednesday 11 March Switzerland &amp;amp; Ghan...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       date                                               text\n",
              "0 2020-03-09 13:49:46+00:00  One need World to make #MAGA .\\nWe need #Paris...\n",
              "1 2020-03-09 13:26:18+00:00  SLOW DOWN\\nGlobal #CO2 emissions from the #pow...\n",
              "2 2020-03-09 13:21:51+00:00  RT @UNDPClimate: This Wednesday 11 March Switz...\n",
              "3 2020-03-09 13:21:42+00:00  RT @UNDPClimate: This Wednesday 11 March Switz...\n",
              "4 2020-03-09 13:20:13+00:00  This Wednesday 11 March Switzerland &amp; Ghan..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryZOYSkXZ_pJ",
        "colab_type": "text"
      },
      "source": [
        "##Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxZ8Lr1l6jT-",
        "colab_type": "code",
        "outputId": "b6c53a46-2aee-44b5-905f-0fb65061236a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Setting stopwords\n",
        "all_stopwords = set(stopwords.words('english'))\n",
        "exclude_stopwords = ['no', 'not']\n",
        "for sw in exclude_stopwords:\n",
        "    all_stopwords.remove(sw)\n",
        "\n",
        "# Initializing Lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def toLowerCase(text):\n",
        "    '''\n",
        "    Returns the text in lowercase.\n",
        "    '''\n",
        "    return text.lower()\n",
        "\n",
        "\n",
        "def removeUrls(text):\n",
        "    # return re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','', text)\n",
        "    return re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b','', text)\n",
        "\n",
        "def removeSpecialChar(text):\n",
        "    '''\n",
        "    Removes special characters which are generally found in tweets.\n",
        "    '''\n",
        "    # Convert @username to empty strings\n",
        "    text = re.sub('@[^\\s]+', '', text)\n",
        "\n",
        "    # Remove 'RT' from retweets\n",
        "    text = re.sub(r'\\brt\\b', '', text)\n",
        "\n",
        "    # Remove additional white spaces\n",
        "    text = re.sub('[\\s]+', ' ', text)\n",
        "\n",
        "    # Replace #hastags with word\n",
        "    text = re.sub(r'#([^\\s]+)', r'\\1', text)\n",
        "\n",
        "    # Trims the tweet\n",
        "    text = text.strip('\\'\"')\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def removeNonAlpha(text):\n",
        "    '''\n",
        "    Remove all characters which are not alphabets, numbers or whitespaces.\n",
        "    '''\n",
        "    return re.sub('[^A-Za-z0-9 ]+', '', text)\n",
        "\n",
        "\n",
        "def removeStopWords(text):\n",
        "    # Remove stop words using NLTK's list of stop words in the corpus module. \n",
        "    tokens_without_sw = [word for word in word_tokenize(text) if not word in all_stopwords]\n",
        "\n",
        "    return(' '.join(tokens_without_sw))\n",
        "\n",
        "\n",
        "def handleEmojis(text):\n",
        "    '''\n",
        "    This function handles sentiments expressed through emoticons by using Python's 'emoji package'.\n",
        "    Replace all emoticons with the expression they represent in plain English.\n",
        "    '''\n",
        "    text = emoji.demojize(text)\n",
        "    text = text.replace(\":\",\" \")\n",
        "    text = text.replace(\"_\",\" \")\n",
        "    text = ' '.join(text.split())\n",
        "    return text\n",
        "\n",
        "\n",
        "def handleContractions(text):\n",
        "    '''\n",
        "    expand shortened words, e.g. convert \"don't\" to \"do not\"\n",
        "    '''\n",
        "    contracted_tokens = [contractions_dict[word] if word in contractions_dict else word for word in word_tokenize(text)]\n",
        "    return \" \".join(contracted_tokens)\n",
        "\n",
        "\n",
        "def get_wordnet_pos(word):\n",
        "    '''\n",
        "    Map POS tag to the format wordnet lemmatizer would accept. Returning 'NOUN' as default tag.\n",
        "    ADJ: adjective,    ADP: adposition,    ADV: adverb,    CONJ: conjunction\t\n",
        "    DET: determiner,article,    NOUN: noun,    NUM: numeral,    PRT: particle\n",
        "    PRON: pronoun,    VERB: verb,    .: punctuation marks\t. , ; !,    X: other\t\n",
        "    '''\n",
        "    tag = pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wn.ADJ,\n",
        "                \"N\": wn.NOUN,\n",
        "                \"V\": wn.VERB,\n",
        "                \"R\": wn.ADV}\n",
        "\n",
        "    return tag_dict.get(tag, wn.NOUN)\n",
        "\n",
        "\n",
        "def wordLemmatization(text):\n",
        "    '''\n",
        "    Lemmatizing the text with the appropriate POS tag\n",
        "    result: ['he', 'kept', 'eat', 'while', 'we', 'be', 'talk']\n",
        "    '''\n",
        "    lemma_tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in word_tokenize(text)]\n",
        "    return \" \".join(lemma_tokens)\n",
        "\n",
        "def processTweets(tweet):\n",
        "    '''\n",
        "    Process the raw tweets and convert them into usable data for sentiment analysis. \n",
        "    '''\n",
        "    # Convert to lower case\n",
        "    tweet = toLowerCase(tweet)\n",
        "\n",
        "    # Remove urls\n",
        "    tweet = removeUrls(tweet)\n",
        "    \n",
        "    # Replacing contractions with full words\n",
        "    tweet = handleContractions(tweet)\n",
        "    \n",
        "    # Replace emoticons with word expressions\n",
        "    tweet = handleEmojis(tweet)\n",
        "    \n",
        "    # Remove all Special characters\n",
        "    tweet = removeSpecialChar(tweet)\n",
        "    \n",
        "    # Remove non alpha characters\n",
        "    tweet = removeNonAlpha(tweet)\n",
        "    \n",
        "    # converting a word to its base form\n",
        "    tweet = wordLemmatization(tweet)\n",
        "    \n",
        "    # Remove stop words\n",
        "    tweet = removeStopWords(tweet)\n",
        "\n",
        "    return tweet\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    tweets = tweets_df['text']\n",
        "    processed_tweets = []\n",
        "    processed_df = pd.DataFrame(columns=['date','text','clean_text'])\n",
        "    for tweet in tweets:\n",
        "        processed_tweets.append(processTweets(tweet))\n",
        "    processed_df['date'] = tweets_df['date']\n",
        "    processed_df['text'] = tweets_df['text']\n",
        "    processed_df['clean_text'] = processed_tweets\n",
        "    print(\"Executed Successfully\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Executed Successfully\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAEhvgBpZyEF",
        "colab_type": "text"
      },
      "source": [
        "###Save processed dataframe for easy reusability"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psHIsC49kJgi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "processed_df_out = open(\"/content/drive/My Drive/dataScience/projects/twitterSentimentAnalysis/dataset/processed_df.pickle\",\"wb\")\n",
        "pickle.dump(processed_df, processed_df_out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0Pj9G5DZt5r",
        "colab_type": "code",
        "outputId": "5f076cfa-3383-4706-8d53-63eb2a47df12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "processed_df.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>text</th>\n",
              "      <th>clean_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2020-03-09 13:49:46+00:00</td>\n",
              "      <td>One need World to make #MAGA .\\nWe need #Paris...</td>\n",
              "      <td>one need world make maga need parisagreement m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2020-03-09 13:26:18+00:00</td>\n",
              "      <td>SLOW DOWN\\nGlobal #CO2 emissions from the #pow...</td>\n",
              "      <td>slow global co2 emission power sector fell 2 l...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2020-03-09 13:21:51+00:00</td>\n",
              "      <td>RT @UNDPClimate: This Wednesday 11 March Switz...</td>\n",
              "      <td>undpclimate wednesday 11 march switzerland amp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2020-03-09 13:21:42+00:00</td>\n",
              "      <td>RT @UNDPClimate: This Wednesday 11 March Switz...</td>\n",
              "      <td>undpclimate wednesday 11 march switzerland amp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2020-03-09 13:20:13+00:00</td>\n",
              "      <td>This Wednesday 11 March Switzerland &amp;amp; Ghan...</td>\n",
              "      <td>wednesday 11 march switzerland amp ghana share...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       date  ...                                         clean_text\n",
              "0 2020-03-09 13:49:46+00:00  ...  one need world make maga need parisagreement m...\n",
              "1 2020-03-09 13:26:18+00:00  ...  slow global co2 emission power sector fell 2 l...\n",
              "2 2020-03-09 13:21:51+00:00  ...  undpclimate wednesday 11 march switzerland amp...\n",
              "3 2020-03-09 13:21:42+00:00  ...  undpclimate wednesday 11 march switzerland amp...\n",
              "4 2020-03-09 13:20:13+00:00  ...  wednesday 11 march switzerland amp ghana share...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Flf8tXR-aJFi",
        "colab_type": "text"
      },
      "source": [
        "## Word Embedding and Reaplcing words with centroid of the clusters of word vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RgRSGeM3ZSJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import word2vec model\n",
        "from gensim.models import Word2Vec, KeyedVectors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdb9xOEx4dN1",
        "colab_type": "code",
        "outputId": "34d2ed18-84d0-40f9-ad08-8deb8b8511bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "model = KeyedVectors.load_word2vec_format('/content/drive/My Drive/dataScience/projects/twitterSentimentAnalysis/dataset/GoogleNews-vectors-negative300.bin.gz', binary=True)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrDAO10yAD4w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = processed_df['clean_text']\n",
        "sentences = [word_tokenize(tweet) for tweet in train_data]\n",
        "\n",
        "words_filtered = []\n",
        "word_vectors = []\n",
        "\n",
        "for words in sentences:\n",
        "    for word in words:\n",
        "        if(word in model.vocab):\n",
        "            word_vectors.append(model[word])\n",
        "            words_filtered.append(word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNepE44pEI1J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# word_centroid_map = pd.DataFrame(np.array(vector_list), words_filtered)\n",
        "# word_centroid_map.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xur0ttE0AFmj",
        "colab_type": "code",
        "outputId": "8724efac-ed55-4443-b85d-3ba60c3ef0af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "word_vec_dict = dict(zip(words_filtered, word_vectors))\n",
        "len(word_vec_dict)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2089"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bpq0TuEBDWf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pickle_out = open(\"/content/drive/My Drive/dataScience/projects/twitterSentimentAnalysis/dataset/word_vec_dict.pickle\",\"wb\")\n",
        "pickle.dump(word_vec_dict, pickle_out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RZxQ3aPAJhf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pickle_in = open(\"/content/drive/My Drive/dataScience/projects/twitterSentimentAnalysis/dataset/word_vec_dict.pickle\", \"rb\")\n",
        "word_vec_dict = pickle.load(pickle_in)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8Ss-MpZxyDn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_vectors = list(word_vec_dict.values())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_5TF7zoOe_q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import pairwise_distances_argmin_min"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMqiBY-Tx814",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_of_clusters = len(word_vectors)//3\n",
        "\n",
        "# create kmeans object\n",
        "kmeans = KMeans(n_clusters=num_of_clusters, random_state=42)\n",
        "\n",
        "# fit kmeans object to data\n",
        "km = kmeans.fit(word_vectors)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWHJQ_6hyAD3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # save new clusters\n",
        "# y_km = kmeans.fit_predict(word_vectors)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPzv6A1YIauU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_centroid_dict = {}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGa2iiyWIPqh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_centroid_dict = dict(zip(list(word_vec_dict.keys()), list(kmeans.fit_predict(word_vectors))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b55tVtyrHS_n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "centroid_words_dict = {}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kl9U7dh5AuKV",
        "colab_type": "code",
        "outputId": "880480c6-caca-48ea-b9eb-008fbf0a8758",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "closest, _ = pairwise_distances_argmin_min(km.cluster_centers_, word_vectors)\n",
        "for i in range(num_of_clusters-1):\n",
        "  your_word_vector = word_vectors[closest[i]]\n",
        "  centroid_words_dict[i] = model.most_similar(positive=[your_word_vector], topn=1)[0][0]"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygDw8Kycdwiq",
        "colab_type": "code",
        "outputId": "68f7222c-6d19-4f16-8fc9-5b4a183b74ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "num_of_clusters, len(word_vectors), len(closest)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(696, 2089, 696)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAGqQJQ6m4wy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1ce9590e-fd26-4d6c-ae14-c5c725d5bbf3"
      },
      "source": [
        "centroid_words_dict.get(word_centroid_dict.get('light'))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'coalfired'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXC8CoyVe0Q9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "ab720588-c1f9-45e3-82a9-10ac15f036dc"
      },
      "source": [
        "def replace_words(text):\n",
        "    repalced_words = []\n",
        "    sent_token = sent_tokenize(text)\n",
        "    word_token = [word_tokenize(sent) for sent in sent_token]\n",
        "    for token in word_token[0]:\n",
        "        replaced_word = centroid_words_dict.get(word_centroid_dict.get(token))\n",
        "        if(replaced_word is None):\n",
        "            replaced_word = token\n",
        "        repalced_words.append(replaced_word)\n",
        "    return repalced_words\n",
        "\n",
        "replaced_words_with_embeddings = []\n",
        "tweets = processed_df['clean_text']\n",
        "# for i in range(1):\n",
        "#     replaced_words_with_embeddings.append(replace_words(tweets[i]))\n",
        "for tweet in tweets:\n",
        "    replaced_words_with_embeddings.append(\" \".join(replace_words(tweet)))\n",
        "processed_df['replaced_words_by_word_embedding'] = replaced_words_with_embeddings\n",
        "processed_df.head()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>text</th>\n",
              "      <th>clean_text</th>\n",
              "      <th>replaced_words_by_word_embedding</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2020-03-09 13:49:46+00:00</td>\n",
              "      <td>One need World to make #MAGA .\\nWe need #Paris...</td>\n",
              "      <td>one need world make maga need parisagreement m...</td>\n",
              "      <td>one want world bring jamal want parisagreement...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2020-03-09 13:26:18+00:00</td>\n",
              "      <td>SLOW DOWN\\nGlobal #CO2 emissions from the #pow...</td>\n",
              "      <td>slow global co2 emission power sector fell 2 l...</td>\n",
              "      <td>fast global co2 emission electricity market fe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2020-03-09 13:21:51+00:00</td>\n",
              "      <td>RT @UNDPClimate: This Wednesday 11 March Switz...</td>\n",
              "      <td>undpclimate wednesday 11 march switzerland amp...</td>\n",
              "      <td>undpclimate wednesday 11 march poland amp ugan...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2020-03-09 13:21:42+00:00</td>\n",
              "      <td>RT @UNDPClimate: This Wednesday 11 March Switz...</td>\n",
              "      <td>undpclimate wednesday 11 march switzerland amp...</td>\n",
              "      <td>undpclimate wednesday 11 march poland amp ugan...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2020-03-09 13:20:13+00:00</td>\n",
              "      <td>This Wednesday 11 March Switzerland &amp;amp; Ghan...</td>\n",
              "      <td>wednesday 11 march switzerland amp ghana share...</td>\n",
              "      <td>wednesday 11 march poland amp uganda itc progr...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       date  ...                   replaced_words_by_word_embedding\n",
              "0 2020-03-09 13:49:46+00:00  ...  one want world bring jamal want parisagreement...\n",
              "1 2020-03-09 13:26:18+00:00  ...  fast global co2 emission electricity market fe...\n",
              "2 2020-03-09 13:21:51+00:00  ...  undpclimate wednesday 11 march poland amp ugan...\n",
              "3 2020-03-09 13:21:42+00:00  ...  undpclimate wednesday 11 march poland amp ugan...\n",
              "4 2020-03-09 13:20:13+00:00  ...  wednesday 11 march poland amp uganda itc progr...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ng5CnCNaZ5B",
        "colab_type": "text"
      },
      "source": [
        "#Score Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lc_vz7f1aZZG",
        "colab_type": "code",
        "outputId": "077447bd-b1ba-4dbd-875c-249d8a2d4fd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "def get_wordnet_pos(word):\n",
        "    '''\n",
        "    Convert the PennTreebank tags to simple Wordnet tags\n",
        "    '''\n",
        "    tag = pos_tag([word])[0][1][0].upper()\n",
        "\n",
        "    tag_dict = {\"J\": wn.ADJ,\n",
        "                \"N\": wn.NOUN,\n",
        "                \"V\": wn.VERB,\n",
        "                \"R\": wn.ADV}\n",
        "\n",
        "    return tag_dict.get(tag, wn.NOUN)\n",
        "\n",
        "\n",
        "def sentimentAnalyser(text):\n",
        "    '''\n",
        "    This Function generates normalized sentiment score and a sentiment category to the tweet.\n",
        "        Score is normalized for fairer comparison.\n",
        "    '''\n",
        "    # to count no. of words which bear scores\n",
        "    num_of_words = 0\n",
        "    sentiment_score = 0\n",
        "    final_score = 0\n",
        "\n",
        "    sent_token = sent_tokenize(text)\n",
        "    word_token = [word_tokenize(sent) for sent in sent_token]\n",
        "    word_pos_tag = [pos_tag(word) for word in word_token]\n",
        "\n",
        "    for i in range(len(word_pos_tag)):\n",
        "        for word, tag in (word_pos_tag[i]):\n",
        "            wn_tag = get_wordnet_pos(tag)\n",
        "\n",
        "            synsets = wn.synsets(word, pos=wn_tag)\n",
        "\n",
        "            if not synsets:\n",
        "                continue\n",
        "\n",
        "            word_sent_score = 0\n",
        "            synset_count = 0\n",
        "            # Calculate the aveage sentiment score of first 5 synsets of given word\n",
        "            for synset in synsets:\n",
        "                synset_count += 1\n",
        "                swn_synset = swn.senti_synset(synset.name())\n",
        "                word_sent_score += swn_synset.pos_score() - swn_synset.neg_score()\n",
        "                num_of_words += 1\n",
        "                if(synset_count==5):\n",
        "                    continue\n",
        "\n",
        "            sentiment_score += word_sent_score\n",
        "    if(num_of_words==0):\n",
        "      normalized_score = 0\n",
        "    else:\n",
        "      normalized_score = round(sentiment_score/num_of_words, 3)\n",
        "\n",
        "    # Classify reviews according to setiment score assigned\n",
        "    # 1 : positive, 0 : neutral, -1 : negative\n",
        "    if normalized_score > 0:\n",
        "        final_score = 1\n",
        "    elif normalized_score < 0:\n",
        "        final_score = -1\n",
        "    elif normalized_score == 0:\n",
        "        final_score = 0\n",
        "\n",
        "    return final_score\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    processed_df\n",
        "    processed_tweets = processed_df['clean_text']\n",
        "    scores = []\n",
        "    for tweet in processed_tweets:\n",
        "        scores.append(sentimentAnalyser(tweet))\n",
        "    processed_df['score'] = scores\n",
        "    # tweet = '''next establish question use question mark liberally indicate question think face indicate not understand \n",
        "    # say question compose 3040 communication critical emoji discovery\n",
        "    # '''\n",
        "    # tweet_with_score = sentimentAnalyser(tweet)\n",
        "    # print(tweet_with_score)\n",
        "    processed_word_embedding_tweets = processed_df['replaced_words_by_word_embedding']\n",
        "    word_embedding_scores = []\n",
        "    for tweet in processed_word_embedding_tweets:\n",
        "        word_embedding_scores.append(sentimentAnalyser(tweet))\n",
        "    processed_df['word_embedding_scores'] = word_embedding_scores\n",
        "    print(\"Executed Successfully\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Executed Successfully\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFLH1Kgy_Nkc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "outputId": "fd8151a3-7033-4c99-e1ba-349d61f816a3"
      },
      "source": [
        "processed_df.head()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>text</th>\n",
              "      <th>clean_text</th>\n",
              "      <th>replaced_words_by_word_embedding</th>\n",
              "      <th>score</th>\n",
              "      <th>word_embedding_scores</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2020-03-09 13:49:46+00:00</td>\n",
              "      <td>One need World to make #MAGA .\\nWe need #Paris...</td>\n",
              "      <td>one need world make maga need parisagreement m...</td>\n",
              "      <td>one want world bring jamal want parisagreement...</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2020-03-09 13:26:18+00:00</td>\n",
              "      <td>SLOW DOWN\\nGlobal #CO2 emissions from the #pow...</td>\n",
              "      <td>slow global co2 emission power sector fell 2 l...</td>\n",
              "      <td>fast global co2 emission electricity market fe...</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2020-03-09 13:21:51+00:00</td>\n",
              "      <td>RT @UNDPClimate: This Wednesday 11 March Switz...</td>\n",
              "      <td>undpclimate wednesday 11 march switzerland amp...</td>\n",
              "      <td>undpclimate wednesday 11 march poland amp ugan...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2020-03-09 13:21:42+00:00</td>\n",
              "      <td>RT @UNDPClimate: This Wednesday 11 March Switz...</td>\n",
              "      <td>undpclimate wednesday 11 march switzerland amp...</td>\n",
              "      <td>undpclimate wednesday 11 march poland amp ugan...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2020-03-09 13:20:13+00:00</td>\n",
              "      <td>This Wednesday 11 March Switzerland &amp;amp; Ghan...</td>\n",
              "      <td>wednesday 11 march switzerland amp ghana share...</td>\n",
              "      <td>wednesday 11 march poland amp uganda itc progr...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       date  ... word_embedding_scores\n",
              "0 2020-03-09 13:49:46+00:00  ...                     1\n",
              "1 2020-03-09 13:26:18+00:00  ...                     1\n",
              "2 2020-03-09 13:21:51+00:00  ...                     1\n",
              "3 2020-03-09 13:21:42+00:00  ...                     1\n",
              "4 2020-03-09 13:20:13+00:00  ...                     1\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QPctcdzOtEJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "processed_df_out = open(\"/content/drive/My Drive/dataScience/projects/twitterSentimentAnalysis/dataset/processed_df.pickle\",\"wb\")\n",
        "pickle.dump(processed_df, processed_df_out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkWbuPjLO10Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "processed_df_in = open(\"/content/drive/My Drive/dataScience/projects/twitterSentimentAnalysis/dataset/processed_df.pickle\", \"rb\")\n",
        "processed_df = pickle.load(processed_df_in)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ha0gXUZqUUm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "processed_df.to_csv(\"/content/drive/My Drive/dataScience/projects/twitterSentimentAnalysis/dataset/results.csv\", index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZQKE7QQiyx2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 726
        },
        "outputId": "fac08218-9aab-4181-8cbe-387ccc98e43a"
      },
      "source": [
        "for n in range(0,100,20):\n",
        "    print(\"date:\", processed_df[\"date\"][n] )\n",
        "    print(\"original_tweet:\", processed_df[\"text\"][n])\n",
        "    print(\"processed_tweet:\", processed_df[\"clean_text\"][n])\n",
        "    print(\"replaced_centroids_tweet:\", processed_df[\"replaced_words_by_word_embedding\"][n])\n",
        "    print(\"sentiment_score:\", processed_df[\"score\"][n])\n",
        "    print(\"sentiment_score_with_embeddings:\", processed_df[\"word_embedding_scores\"][n],'\\n')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "date: 2020-03-09 13:49:46+00:00\n",
            "original_tweet: One need World to make #MAGA .\n",
            "We need #ParisAgreement to make Earth Great.\n",
            "@UNEP @UN @IPCC_CH @SciNetUCS… https://t.co/BJhy83Oyaf\n",
            "processed_tweet: one need world make maga need parisagreement make earth great unep un ipcc ch scinetucs\n",
            "replaced_centroids_tweet: one want world bring jamal want parisagreement bring planet good unep schwarzenegger ipcc h scinetucs\n",
            "sentiment_score: -1\n",
            "sentiment_score_with_embeddings: 1 \n",
            "\n",
            "date: 2020-03-09 11:36:09+00:00\n",
            "original_tweet: RT @PinkPetro: \"Millions of minds and souls insisted upon a single, solitary truth: the #ParisAgreement is historic, essential, and here to…\n",
            "processed_tweet: pinkpetro million mind soul insist upon single solitary truth parisagreement historic essential\n",
            "replaced_centroids_tweet: pinkpetro million actually ndc argue one another one indeed parisagreement historic crucial\n",
            "sentiment_score: 1\n",
            "sentiment_score_with_embeddings: -1 \n",
            "\n",
            "date: 2020-03-09 09:15:46+00:00\n",
            "original_tweet: RT @JudithHartmann_: I am verry happy to share that @ENGIEgroup has recently been certified #SBT @sciencetargets, being the first multi-ene…\n",
            "processed_tweet: judithhartmann verry happy share engiegroup recently certify sbt sciencetargets first multiene\n",
            "replaced_centroids_tweet: judithhartmann jamal glad itc engiegroup last certify sbt sciencetargets first multiene\n",
            "sentiment_score: 1\n",
            "sentiment_score_with_embeddings: 1 \n",
            "\n",
            "date: 2020-03-09 06:05:20+00:00\n",
            "original_tweet: FYI @POTUS.\n",
            "#ParisAgreement is all about learning  \" how to care our Mother Earth\".\n",
            "It's a #ClimateAction on… https://t.co/opaYNDVXOT\n",
            "processed_tweet: fyi potus parisagreement learn care mother earth climateaction\n",
            "replaced_centroids_tweet: johnston soros parisagreement learn health woman planet climateaction\n",
            "sentiment_score: -1\n",
            "sentiment_score_with_embeddings: 1 \n",
            "\n",
            "date: 2020-03-09 04:01:40+00:00\n",
            "original_tweet: RT @RichieMerzian: The most senior UN official in charge of the development and adoption of the #ParisAgreement offloaded on the Aus Gov \"i…\n",
            "processed_tweet: richiemerzian senior un official charge development adoption parisagreement offload au gov\n",
            "replaced_centroids_tweet: richiemerzian ndc schwarzenegger official itc develop adoption parisagreement sell au gov\n",
            "sentiment_score: -1\n",
            "sentiment_score_with_embeddings: -1 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNo9Z1qt91g_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c5bf5f61-8289-4940-f30e-c7acff92b17f"
      },
      "source": [
        "length = len(processed_df)\n",
        "count = 0\n",
        "for i in range(len(processed_df)):\n",
        "    if(processed_df['score'][i] != processed_df['word_embedding_scores'][i]):\n",
        "        count+=1\n",
        "print(\"total tweets: {} same score: {} different score: {}\".format(length, length - count, count))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total tweets: 3456 same score: 2307 different score: 1149\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}