{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Environmental Hashtag Positive/Negative Scoring System</h1>\n",
    "<p><u><b>Objective:</b></u> Add a positive or negative comment to environmental comments from Twitter</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><u>Import all the necessary libraries</u></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We add all the imports\n",
    "import json # Import json to handle json files.\n",
    "from nltk.corpus import wordnet  #Import wordnet from the NLTK\n",
    "from nltk.corpus import sentiwordnet as swn # We import sentiwordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><u>Create Data Cleaning Methods</u></h2>\n",
    "<p>Created by <b>@Chhandosee Bhattacharya<b></p>\n",
    "<p>References: <a href=https://medium.com/analytics-vidhya/working-with-twitter-data-b0aa5419532>link</a><p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries needed for data cleaning\n",
    "import emoji\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function cleans the data and makes it ready for Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Data_Cleaning(lines):\n",
    "    # Define a dictioary of emoticons\n",
    "    dict_emo = { ':-)'  : b'\\xf0\\x9f\\x98\\x8a',\n",
    "                      ':)'   : b'\\xf0\\x9f\\x98\\x8a',\n",
    "                      '=)'   : b'\\xf0\\x9f\\x98\\x8a',  # Smile or happy\n",
    "                     ':-D'  : b'\\xf0\\x9f\\x98\\x83',\n",
    "                      ':D'   : b'\\xf0\\x9f\\x98\\x83',\n",
    "                      '=D'   : b'\\xf0\\x9f\\x98\\x83',  # Big smile\n",
    "                      '>:-(' : b'\\xF0\\x9F\\x98\\xA0',\n",
    "                      '>:-o' : b'\\xF0\\x9F\\x98\\xA0'   # Angry face\n",
    "                      }\n",
    "    #Function converts emoticons to emoji\n",
    "    def convert_emoticons(emoticons):\n",
    "        emoticons=emoticons.replace('.',' ')\n",
    "        emoticons=emoticons.replace(',',' ')\n",
    "        for i in emoticons.split():\n",
    "            if i in dict_emo.keys():\n",
    "                word=dict_emo[i].decode('utf-8')\n",
    "                emoticons=emoticons.replace(i,word)\n",
    "        return emoticons\n",
    "\n",
    "    #Function to convert emoji to word\n",
    "    def convert_emoji_to_word(emo_converted_text):\n",
    "        for i in emo_converted_text:\n",
    "            if i in emoji.UNICODE_EMOJI:\n",
    "                emo_word=str(emoji.demojize(i))\n",
    "                rep_colon=emo_word.replace(':',' ')\n",
    "                rep_dash=rep_colon.replace('_',' ')\n",
    "                emo_converted_text=emo_converted_text.replace(i,rep_dash)\n",
    "        return emo_converted_text\n",
    "    # Function tokenizes the whole data, normalies it to lower case, removes \n",
    "    #stop words and stems the data\n",
    "    def clean_data(text):\n",
    "        tokens= word_tokenize(text)\n",
    "        # convert to lower case\n",
    "        tokens = [w.lower() for w in tokens]\n",
    "        # remove all tokens that are not alphabetic\n",
    "        words = [word for word in tokens if word.isalpha()]\n",
    "        # filter out stop words\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        words = [w for w in words if not w in stop_words]\n",
    "        porter = PorterStemmer()\n",
    "        stemmed = [porter.stem(word) for word in words]\n",
    "        return stemmed\n",
    "\t# Function to remove hyperlinks from a tweet\n",
    "\t# Created by Otoniel Campos\n",
    "    def remove_hyperlinks(tweet):\n",
    "\t    result = re.sub(r\"http\\S+\", \"\", tweet)\n",
    "\t    return result\n",
    "\n",
    "    # Task of data Cleaning function starts here  \n",
    "    text=str(lines)\n",
    "    text=text.replace(\"ufeff\",\" \")\n",
    "    text=text.replace(\"'\",\"\")\n",
    "    emoticons_treated=convert_emoticons(text)\n",
    "    emo_treated_lines=convert_emoji_to_word(emoticons_treated)\n",
    "    removed_hyperlinks=remove_hyperlinks(emo_treated_lines)\n",
    "    cleaned_text=clean_data(removed_hyperlinks)\n",
    "    # The final cleaned text is ready for word embedding\n",
    "    return set(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><u>Iterate and assign scores</u></h2>\n",
    "<p>We iterate through all the files and assign a positive/negative score to each tweet.</p>\n",
    "<p><b>tweet_score</b> = negative_score - positive_score; negative_score and positive_score are both integers >= 0.</p>\n",
    "<ol>\n",
    "    <li>A &gt;0 overall_score means a negative sentiment, while a &lt;0 overall_score corresponds to a negative sentiment.</li>\n",
    "    <li>A 0 overall_score reflects a neutral sentiment.</li>\n",
    "</ol>\n",
    "<p>At the end we will create a new files that will contain the scores of each tweet.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><u>Implementation</u></h4>\n",
    "<p>Below we show the implementation of the functions that were needed to calculate a score to each tweet</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Get Hypernyms function</h4>\n",
    "<p>The following function gets the hypernyms of each word in a tweet.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hypernyms(tweet):\n",
    "\n",
    "    new_tweet = tweet\n",
    "\n",
    "    # We create a counter to iterate through each word\n",
    "    counter = 0\n",
    "    # Now we iterate through each word to get the\n",
    "    for w in tweet:\n",
    "        synset = wordnet.synsets(w)\n",
    "\n",
    "        # We check the length of the synset; we continue only if we get a response from wordnet.synsets\n",
    "        if len(synset) > 0:\n",
    "            # We get the hypernym\n",
    "            hyper = synset[0].hypernyms()\n",
    "            # If we get a result\n",
    "            if len(hyper) > 0:\n",
    "                first_lemma = hyper[0]\n",
    "                lemmas = first_lemma.lemma_names()\n",
    "                new_tweet[counter] = lemmas[0]\n",
    "\n",
    "        # We increase the counter\n",
    "        counter += 1\n",
    "\n",
    "    return new_tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Get Tweet Scores function</h4>\n",
    "<p>This function returns the score of the tweet</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweet_score(s):\n",
    "    # We split the tweet by spaces to get each individual word.\n",
    "    split_text = s.split()\n",
    "\n",
    "    # We start by assigning 0 to the sentiment_score, this sentiment_score will consist of the sum of all\n",
    "    # sentiment scores for each individual word.\n",
    "    sentiment_score = 0  # sentiment_score\n",
    "\n",
    "    # Here we specify the denominator that we will use to get the average\n",
    "    # We will only take into account the words that have a score in wordnet.synsets\n",
    "    syn_denominator = 0\n",
    "    # We specify the overall score\n",
    "    overall_score = 0\n",
    "\n",
    "    # Now we replace each for in the tweet for its hypernym\n",
    "    split_text = get_hypernyms(split_text)\n",
    "\n",
    "    # Now we iterate through each word to get the\n",
    "    for w in split_text:\n",
    "        synset = wordnet.synsets(w)\n",
    "        # We check the length of the synset; we continue only if we get a response from wordnet.synsets\n",
    "        if len(synset) > 0:\n",
    "            # synset[0].name() contains the id that we will use to reference the word later in senti_synset\n",
    "            # We assign this value to the variable name\n",
    "            name = synset[0].name()\n",
    "            # Below we get the positive and negative scores for the word using its name as id\n",
    "            breakdown = swn.senti_synset(name)\n",
    "            # We get the negative and positive scores\n",
    "            pos_score = breakdown.pos_score()\n",
    "            neg_score = breakdown.neg_score()\n",
    "\n",
    "            # Then we calculate the sentiment_score for this word and add it up to the scores of the previous words.\n",
    "            sentiment_score += neg_score - pos_score  # The sentiment_score of all words in the tweet.\n",
    "\n",
    "            # We increase the syn_denominator count + 1\n",
    "            syn_denominator += 1\n",
    "\n",
    "    # Now we will calculate the average using syn_denominator as denominator\n",
    "    if syn_denominator != 0:\n",
    "        # Now we calculate the average and assign it to the overall_score\n",
    "        # The overall_score will be the mean of each score\n",
    "        overall_score = sentiment_score / syn_denominator\n",
    "\n",
    "    # We return the tweet's overall_score.\n",
    "    return overall_score, split_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Assign scores function</h4>\n",
    "<p>The following is the implementation of assign scores phase.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we iterate through all the files and assign a score\n",
    "def assign_scores(files_list):\n",
    "    for file in files_list:\n",
    "        # All files are txt files.\n",
    "        f = open('Datasets/' + file + '.txt')\n",
    "        # We read the lines of the txt file.\n",
    "        lines = f.readlines()\n",
    "\n",
    "        # We create the sentences dictionary. This will contain an array of sentences which each element will contain the\n",
    "        # text and an overall score for the whole sentence.\n",
    "        sentences = []\n",
    "\n",
    "        # Counter for each sentence in the corpus, this is for later comparison with the file where we extracted the data.\n",
    "        count = 0\n",
    "\n",
    "        # For each line in the file we will proceed to\n",
    "        for line in lines:\n",
    "            # Data Extraction\n",
    "            # We parse each fine of the file.\n",
    "            parsed_json = (json.loads(line))\n",
    "\n",
    "            # Discriminate to only take english sentences\n",
    "            if parsed_json['lang'] == 'en':\n",
    "                # We extract the text from the parsed line\n",
    "                text = parsed_json['text']\n",
    "\n",
    "                # Now we proceed to clean the text\n",
    "                cleaned_set = Data_Cleaning(text)\n",
    "                after_clean_text = \" \".join(cleaned_set)\n",
    "\n",
    "                # Now we iterate and we take the hypernyms of each word\n",
    "                # We get the score for each sentence\n",
    "                score, hypernyms = get_tweet_score(after_clean_text)\n",
    "\n",
    "                # Dictionary creation\n",
    "                # We create a dictionary for the sentence\n",
    "                sentences.append({\n",
    "                    'text': text,\n",
    "                    'after_clean_text': after_clean_text,\n",
    "                    'hypernyms': hypernyms,\n",
    "                    'score': score\n",
    "                })\n",
    "\n",
    "            # We increase the counter\n",
    "            count += 1\n",
    "\n",
    "        # We create a new file for each hash tag file that we consulted.\n",
    "        with open('Results/' + file + '_final_' + '.json', 'w') as outfile:\n",
    "            # We finally dump the tweets + the overall_score in a json file.\n",
    "            json.dump(sentences, outfile, indent=2)\n",
    "\n",
    "        # End of the program's execution\n",
    "        print(\"Total lines = \" + str(count) + \" we're processed.\")\n",
    "        print(\"Score assignation has finished successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Output</h4>\n",
    "<p>We open the contents of the first file to check how the output looks like.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines = 10 we're processed.\n",
      "Score assignation has finished successfully!\n",
      "Total lines = 10 we're processed.\n",
      "Score assignation has finished successfully!\n",
      "Total lines = 10 we're processed.\n",
      "Score assignation has finished successfully!\n",
      "[\n",
      "  {\n",
      "    \"text\": \"RT @WantMyMoneyBac: @love4thegameAK Muslim Brothhood written goal is to infiltrate from within. Thanks Obama.\\n\\nBest 4 minutes you\\u2019ll spend\\u2026\",\n",
      "    \"after_clean_text\": \"muslim obama infiltr wantmymoneybac best minut goal brothhood written within rt thank\",\n",
      "    \"hypernyms\": [\n",
      "      \"religious_person\",\n",
      "      \"obama\",\n",
      "      \"infiltr\",\n",
      "      \"wantmymoneybac\",\n",
      "      \"attempt\",\n",
      "      \"minut\",\n",
      "      \"content\",\n",
      "      \"brothhood\",\n",
      "      \"create_verbally\",\n",
      "      \"within\",\n",
      "      \"rt\",\n",
      "      \"convey\"\n",
      "    ],\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"text\": \"RT @RebeccaH2020: We chose @GretaThunberg because she stands up for what she believes\\nChildren at a Yorkshire school chose the \\\"inspiring\\\"\\u2026\",\n",
      "    \"after_clean_text\": \"believ yorkshir school inspir gretathunberg stand chose children rt\",\n",
      "    \"hypernyms\": [\n",
      "      \"believ\",\n",
      "      \"yorkshir\",\n",
      "      \"educational_institution\",\n",
      "      \"inspir\",\n",
      "      \"gretathunberg\",\n",
      "      \"support\",\n",
      "      \"decide\",\n",
      "      \"juvenile\",\n",
      "      \"rt\"\n",
      "    ],\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"text\": \"RT @RebeccaH2020: We chose @GretaThunberg because she stands up for what she believes\\nChildren at a Yorkshire school chose the \\\"inspiring\\\"\\u2026\",\n",
      "    \"after_clean_text\": \"believ yorkshir school inspir gretathunberg stand chose children rt\",\n",
      "    \"hypernyms\": [\n",
      "      \"believ\",\n",
      "      \"yorkshir\",\n",
      "      \"educational_institution\",\n",
      "      \"inspir\",\n",
      "      \"gretathunberg\",\n",
      "      \"support\",\n",
      "      \"decide\",\n",
      "      \"juvenile\",\n",
      "      \"rt\"\n",
      "    ],\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"text\": \"RT @WantMyMoneyBac: @love4thegameAK Muslim Brothhood written goal is to infiltrate from within. Thanks Obama.\\n\\nBest 4 minutes you\\u2019ll spend\\u2026\",\n",
      "    \"after_clean_text\": \"muslim obama infiltr wantmymoneybac best minut goal brothhood written within rt thank\",\n",
      "    \"hypernyms\": [\n",
      "      \"religious_person\",\n",
      "      \"obama\",\n",
      "      \"infiltr\",\n",
      "      \"wantmymoneybac\",\n",
      "      \"attempt\",\n",
      "      \"minut\",\n",
      "      \"content\",\n",
      "      \"brothhood\",\n",
      "      \"create_verbally\",\n",
      "      \"within\",\n",
      "      \"rt\",\n",
      "      \"convey\"\n",
      "    ],\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"text\": \"RT @CAREGlobal: \\\"The climate crisis has a female face.\\n\\nAs a woman from Africa, I am often oppressed by racism, sexism, culture, classism,\\u2026\",\n",
      "    \"after_clean_text\": \"crisi classism africa often oppress femal cultur careglob woman climat rt racism face sexism\",\n",
      "    \"hypernyms\": [\n",
      "      \"crisi\",\n",
      "      \"classism\",\n",
      "      \"africa\",\n",
      "      \"often\",\n",
      "      \"oppress\",\n",
      "      \"femal\",\n",
      "      \"cultur\",\n",
      "      \"careglob\",\n",
      "      \"adult\",\n",
      "      \"climat\",\n",
      "      \"rt\",\n",
      "      \"bias\",\n",
      "      \"external_body_part\",\n",
      "      \"discrimination\"\n",
      "    ],\n",
      "    \"score\": 0.16071428571428573\n",
      "  },\n",
      "  {\n",
      "    \"text\": \"RT @GretaThunberg: On March 13-14th there will be big mobilisations for the climate in France! I will join the climate strike in Grenoble o\\u2026\",\n",
      "    \"after_clean_text\": \"big gretathunberg franc grenobl march strike join climat rt mobilis\",\n",
      "    \"hypernyms\": [\n",
      "      \"big\",\n",
      "      \"gretathunberg\",\n",
      "      \"monetary_unit\",\n",
      "      \"grenobl\",\n",
      "      \"Gregorian_calendar_month\",\n",
      "      \"job_action\",\n",
      "      \"connection\",\n",
      "      \"climat\",\n",
      "      \"rt\",\n",
      "      \"mobilis\"\n",
      "    ],\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"text\": \"RT @RebeccaH2020: We chose @GretaThunberg because she stands up for what she believes\\nChildren at a Yorkshire school chose the \\\"inspiring\\\"\\u2026\",\n",
      "    \"after_clean_text\": \"believ yorkshir school inspir gretathunberg stand chose children rt\",\n",
      "    \"hypernyms\": [\n",
      "      \"believ\",\n",
      "      \"yorkshir\",\n",
      "      \"educational_institution\",\n",
      "      \"inspir\",\n",
      "      \"gretathunberg\",\n",
      "      \"support\",\n",
      "      \"decide\",\n",
      "      \"juvenile\",\n",
      "      \"rt\"\n",
      "    ],\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"text\": \"RT @GretaThunberg: On March 13-14th there will be big mobilisations for the climate in France! I will join the climate strike in Grenoble o\\u2026\",\n",
      "    \"after_clean_text\": \"big gretathunberg franc grenobl march strike join climat rt mobilis\",\n",
      "    \"hypernyms\": [\n",
      "      \"big\",\n",
      "      \"gretathunberg\",\n",
      "      \"monetary_unit\",\n",
      "      \"grenobl\",\n",
      "      \"Gregorian_calendar_month\",\n",
      "      \"job_action\",\n",
      "      \"connection\",\n",
      "      \"climat\",\n",
      "      \"rt\",\n",
      "      \"mobilis\"\n",
      "    ],\n",
      "    \"score\": 0.0\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# We input the list of files\n",
    "files_list = ['tweets_fridaysforfuture_09-03-2020_test', 'tweets_climatechange_06-03-2020_test', 'tweets_savetheplanet_09-03-2020_test']\n",
    "assign_scores(files_list)\n",
    "\n",
    "# We load the url of the first file.\n",
    "url = \"Results/\" + files_list[0] + \".json\"\n",
    "\n",
    "# We will open the file\n",
    "with open(url, 'r') as json_file:\n",
    "    data = json_file.read()\n",
    "\n",
    "# Now we parse the file\n",
    "parsed = json.loads(data)\n",
    "\n",
    "# Pretty Printing JSON string back\n",
    "print(json.dumps(parsed, indent = 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Run on all files.</h4>\n",
    "<p>Below we will run the implemented code on all the test files.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines = 149399 we're processed.\n",
      "Score assignation has finished successfully!\n",
      "Total lines = 42315 we're processed.\n",
      "Score assignation has finished successfully!\n",
      "Total lines = 7385 we're processed.\n",
      "Score assignation has finished successfully!\n"
     ]
    }
   ],
   "source": [
    "new_files_list = ['tweets_climatechange_06-03-2020', 'tweets_fridaysforfuture_09-03-2020', 'tweets_savetheplanet_09-03-2020']\n",
    "assign_scores(new_files_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'RT @southpoleglobal: #IWD2020 this Sunday remind us that solutions to drive down emissions also need to support gender equality. Take a loo…', 'after_clean_text': 'take southpoleglob support equal also need solut sunday drive gender remind rt us emiss', 'hypernyms': ['income', 'southpoleglob', 'activity', 'person', 'also', 'condition', 'solut', 'rest_day', 'propulsion', 'grammatical_category', 'remind', 'rt', 'us', 'emiss'], 'score': -0.0125}\n",
      "\n",
      "{'text': 'Sustainable ADA Signage – Top Six 2019 Trends: https://t.co/j8RxOw34qI #Signage #ADA #Greenbuilding #Architecture… https://t.co/jQqKgZLY2b', 'after_clean_text': 'six ada sustain greenbuild trend top signag', 'hypernyms': ['digit', 'enzyme', 'continue', 'greenbuild', 'direction', 'region', 'signag'], 'score': 0.0}\n",
      "\n",
      "{'text': \"Nori's @CleanEnergyGrl is the third (!) Norinaut to be on @zengineeringpod! Y'all really can't get enough, huh? Che… https://t.co/Hixb10A5ww\", 'after_clean_text': 'norinaut nori zengineeringpod cant get enough huh yall third realli cleanenergygrl', 'hypernyms': ['norinaut', 'nori', 'zengineeringpod', 'nonsense', 'return', 'relative_quantity', 'huh', 'yall', 'common_fraction', 'realli', 'cleanenergygrl'], 'score': 0.09375}\n",
      "\n",
      "{'text': 'RT @alligroberts: Dustin Sarnoski from @StateStreet says that science-based targets are the goal that all companies should be working towar…', 'after_clean_text': 'dustin work statestreet target say alligrobert rt goal compani sarnoski', 'hypernyms': ['dustin', 'activity', 'statestreet', 'reference_point', 'opportunity', 'alligrobert', 'rt', 'content', 'compani', 'sarnoski'], 'score': -0.15625}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We load the url of the first file.\n",
    "url = \"Results/\" + new_files_list[0] + \"_final_\" + \".json\"\n",
    "\n",
    "# JSON file\n",
    "f = open (url, \"r\")\n",
    "\n",
    "# Reading from file\n",
    "data = json.loads(f.read())\n",
    "\n",
    "for i in range(4):\n",
    "\tprint(str(data[i])+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
