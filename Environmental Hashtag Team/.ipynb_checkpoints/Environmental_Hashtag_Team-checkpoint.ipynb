{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Environmental Hashtag Positive/Negative Scoring System</h1>\n",
    "<p><u><b>Objective:</b></u> Add a positive or negative comment to environmental comments from Twitter</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><u>Import all the necessary libraries</u></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We add all the imports\n",
    "import json # Import json to handle json files.\n",
    "from nltk.corpus import wordnet  #Import wordnet from the NLTK\n",
    "from nltk.corpus import sentiwordnet as swn # We import sentiwordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><u>Create Data Cleaning Methods</u></h2>\n",
    "<p>Created by <b>@Chhandosee Bhattacharya<b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries needed for data cleaning\n",
    "import emoji\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function cleans the data and makes it ready for Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Data_Cleaning(lines):\n",
    "    # Define a dictioary of emoticons\n",
    "    dict_emo = { ':-)'  : b'\\xf0\\x9f\\x98\\x8a',\n",
    "                      ':)'   : b'\\xf0\\x9f\\x98\\x8a',\n",
    "                      '=)'   : b'\\xf0\\x9f\\x98\\x8a',  # Smile or happy\n",
    "                     ':-D'  : b'\\xf0\\x9f\\x98\\x83',\n",
    "                      ':D'   : b'\\xf0\\x9f\\x98\\x83',\n",
    "                      '=D'   : b'\\xf0\\x9f\\x98\\x83',  # Big smile\n",
    "                      '>:-(' : b'\\xF0\\x9F\\x98\\xA0',\n",
    "                      '>:-o' : b'\\xF0\\x9F\\x98\\xA0'   # Angry face\n",
    "                      }\n",
    "    #Function converts emoticons to emoji\n",
    "    def convert_emoticons(emoticons):\n",
    "        emoticons=emoticons.replace('.',' ')\n",
    "        emoticons=emoticons.replace(',',' ')\n",
    "        for i in emoticons.split():\n",
    "            if i in dict_emo.keys():\n",
    "                word=dict_emo[i].decode('utf-8')\n",
    "                emoticons=emoticons.replace(i,word)\n",
    "        return emoticons\n",
    "\n",
    "    #Function to convert emoji to word\n",
    "    def convert_emoji_to_word(emo_converted_text):\n",
    "        for i in emo_converted_text:\n",
    "            if i in emoji.UNICODE_EMOJI:\n",
    "                emo_word=str(emoji.demojize(i))\n",
    "                rep_colon=emo_word.replace(':',' ')\n",
    "                rep_dash=rep_colon.replace('_',' ')\n",
    "                emo_converted_text=emo_converted_text.replace(i,rep_dash)\n",
    "        return emo_converted_text\n",
    "    # Function tokenizes the whole data, normalies it to lower case, removes \n",
    "    #stop words and stems the data\n",
    "    def clean_data(text):\n",
    "        tokens= word_tokenize(text)\n",
    "        # convert to lower case\n",
    "        tokens = [w.lower() for w in tokens]\n",
    "        # remove all tokens that are not alphabetic\n",
    "        words = [word for word in tokens if word.isalpha()]\n",
    "        # filter out stop words\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        words = [w for w in words if not w in stop_words]\n",
    "        porter = PorterStemmer()\n",
    "        stemmed = [porter.stem(word) for word in words]\n",
    "        return stemmed\n",
    "\t# Function to remove hyperlinks from a tweet\n",
    "\t# Created by Otoniel Campos\n",
    "    def remove_hyperlinks(tweet):\n",
    "\t    result = re.sub(r\"http\\S+\", \"\", tweet)\n",
    "\t    return result\n",
    "\n",
    "    # Task of data Cleaning function starts here  \n",
    "    text=str(lines)\n",
    "    text=text.replace(\"ufeff\",\" \")\n",
    "    text=text.replace(\"'\",\"\")\n",
    "    emoticons_treated=convert_emoticons(text)\n",
    "    emo_treated_lines=convert_emoji_to_word(emoticons_treated)\n",
    "    removed_hyperlinks=remove_hyperlinks(emo_treated_lines)\n",
    "    cleaned_text=clean_data(removed_hyperlinks)\n",
    "    # The final cleaned text is ready for word embedding\n",
    "    return set(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><u>Iterate and assign scores</u></h2>\n",
    "<p>We iterate through all the files and assign a positive/negative score to each tweet.</p>\n",
    "<p><b>tweet_score</b> = negative_score - positive_score; negative_score and positive_score are both integers >= 0.</p>\n",
    "<ol>\n",
    "    <li>A &gt;0 overall_score means a negative sentiment, while a &lt;0 overall_score corresponds to a negative sentiment.</li>\n",
    "    <li>A 0 overall_score reflects a neutral sentiment.</li>\n",
    "</ol>\n",
    "<p>At the end we will create a new files that will contain the scores of each tweet.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><u>Implementation</u></h4>\n",
    "<p>Below we show the implementation of the functions that were needed to calculate a score to each tweet</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Get Hypernyms function</h4>\n",
    "<p>The following function gets the hypernyms of each word in a tweet.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hypernyms(tweet):\n",
    "\n",
    "    new_tweet = tweet\n",
    "\n",
    "    # We create a counter to iterate through each word\n",
    "    counter = 0\n",
    "    # Now we iterate through each word to get the\n",
    "    for w in tweet:\n",
    "        synset = wordnet.synsets(w)\n",
    "\n",
    "        # We check the length of the synset; we continue only if we get a response from wordnet.synsets\n",
    "        if len(synset) > 0:\n",
    "            # We get the hypernym\n",
    "            hyper = synset[0].hypernyms()\n",
    "            # If we get a result\n",
    "            if len(hyper) > 0:\n",
    "                first_lemma = hyper[0]\n",
    "                lemmas = first_lemma.lemma_names()\n",
    "                new_tweet[counter] = lemmas[0]\n",
    "\n",
    "        # We increase the counter\n",
    "        counter += 1\n",
    "\n",
    "    return new_tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Get Tweet Scores function</h4>\n",
    "<p>This function returns the score of the tweet</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweet_score(s):\n",
    "    # We split the tweet by spaces to get each individual word.\n",
    "    split_text = s.split()\n",
    "\n",
    "    # We start by assigning 0 to the sentiment_score, this sentiment_score will consist of the sum of all\n",
    "    # sentiment scores for each individual word.\n",
    "    sentiment_score = 0  # sentiment_score\n",
    "\n",
    "    # Here we specify the denominator that we will use to get the average\n",
    "    # We will only take into account the words that have a score in wordnet.synsets\n",
    "    syn_denominator = 0\n",
    "    # We specify the overall score\n",
    "    overall_score = 0\n",
    "\n",
    "    # Now we replace each for in the tweet for its hypernym\n",
    "    split_text = get_hypernyms(split_text)\n",
    "\n",
    "    # Now we iterate through each word to get the\n",
    "    for w in split_text:\n",
    "        synset = wordnet.synsets(w)\n",
    "        # We check the length of the synset; we continue only if we get a response from wordnet.synsets\n",
    "        if len(synset) > 0:\n",
    "            # synset[0].name() contains the id that we will use to reference the word later in senti_synset\n",
    "            # We assign this value to the variable name\n",
    "            name = synset[0].name()\n",
    "            # Below we get the positive and negative scores for the word using its name as id\n",
    "            breakdown = swn.senti_synset(name)\n",
    "            # We get the negative and positive scores\n",
    "            pos_score = breakdown.pos_score()\n",
    "            neg_score = breakdown.neg_score()\n",
    "\n",
    "            # Then we calculate the sentiment_score for this word and add it up to the scores of the previous words.\n",
    "            sentiment_score += neg_score - pos_score  # The sentiment_score of all words in the tweet.\n",
    "\n",
    "            # We increase the syn_denominator count + 1\n",
    "            syn_denominator += 1\n",
    "\n",
    "    # Now we will calculate the average using syn_denominator as denominator\n",
    "    if syn_denominator != 0:\n",
    "        # Now we calculate the average and assign it to the overall_score\n",
    "        # The overall_score will be the mean of each score\n",
    "        overall_score = sentiment_score / syn_denominator\n",
    "\n",
    "    # We return the tweet's overall_score.\n",
    "    return overall_score, split_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Assign scores function</h4>\n",
    "<p>The following is the implementation of assign scores phase.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines = 8 we're processed.\n",
      "Score assignation has finished successfully!\n"
     ]
    }
   ],
   "source": [
    "# Next we iterate through all the files and assign a score\n",
    "def assign_scores(files_list):\n",
    "\tfor file in files_list:\n",
    "\t\t# All files are txt files.\n",
    "\t\tf = open('Datasets/' + file + '.txt')\n",
    "\t\t# We read the lines of the txt file.\n",
    "\t\tlines = f.readlines()\n",
    "\n",
    "\t\t# We create the sentences dictionary. This will contain an array of sentences which each element will contain the\n",
    "\t\t# text and an overall score for the whole sentence.\n",
    "\t\tsentences = []\n",
    "\n",
    "\t\t# Counter for each sentence in the corpus, this is for later comparison with the file where we extracted the data.\n",
    "\t\tcount = 0\n",
    "\n",
    "\t\t# For each line in the file we will proceed to\n",
    "\t\tfor line in lines:\n",
    "\t\t\t# Data Extraction\n",
    "\t\t\t# We parse each fine of the file.\n",
    "\t\t\tparsed_json = (json.loads(line))\n",
    "\n",
    "\t\t\t# Discriminate to only take english sentences\n",
    "\t\t\tif parsed_json['lang'] == 'en':\n",
    "\t\t\t\t# We extract the text from the parsed line\n",
    "\t\t\t\ttext = parsed_json['text']\n",
    "\n",
    "\t\t\t\t# Now we proceed to clean the text\n",
    "\t\t\t\tcleaned_set = Data_Cleaning(text)\n",
    "\t\t\t\tafter_clean_text = \" \".join(cleaned_set)\n",
    "\n",
    "\t\t\t\t# Now we iterate and we take the hypernyms of each word\n",
    "\t\t\t\t# We get the score for each sentence\n",
    "\t\t\t\tscore, hypernyms = get_tweet_score(after_clean_text)\n",
    "\n",
    "\t\t\t\t# Dictionary creation\n",
    "\t\t\t\t# We create a dictionary for the sentence\n",
    "\t\t\t\tsentences.append({\n",
    "\t\t\t\t\t'text': text,\n",
    "\t\t\t\t\t'after_clean_text': after_clean_text,\n",
    "\t\t\t\t\t'hypernyms': hypernyms,\n",
    "\t\t\t\t\t'score': score\n",
    "\t\t\t\t})\n",
    "\n",
    "\t\t\t\t# We increase the counter\n",
    "\t\t\t\tcount += 1\n",
    "\n",
    "\t# We create a new file for each hash tag file that we consulted.\n",
    "\twith open('Results/' + file + '.json', 'w') as outfile:\n",
    "\t\t# We finally dump the tweets + the overall_score in a json file.\n",
    "\t\tjson.dump(sentences, outfile, indent=2)\n",
    "\n",
    "\t# End of the program's execution\n",
    "\tprint(\"Total lines = \" + str(count) + \" we're processed.\")\n",
    "\tprint(\"Score assignation has finished successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Output</h4>\n",
    "<p>We open the contents of the first file to check how the output looks like.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"text\": \"RT @southpoleglobal: #IWD2020 this Sunday remind us that solutions to drive down emissions also need to support gender equality. Take a loo\\u2026\",\n",
      "    \"after_clean_text\": \"support take us gender also need drive emiss southpoleglob equal rt sunday solut remind\",\n",
      "    \"hypernyms\": [\n",
      "      \"activity\",\n",
      "      \"income\",\n",
      "      \"us\",\n",
      "      \"grammatical_category\",\n",
      "      \"also\",\n",
      "      \"condition\",\n",
      "      \"propulsion\",\n",
      "      \"emiss\",\n",
      "      \"southpoleglob\",\n",
      "      \"person\",\n",
      "      \"rt\",\n",
      "      \"rest_day\",\n",
      "      \"solut\",\n",
      "      \"remind\"\n",
      "    ],\n",
      "    \"score\": -0.0125\n",
      "  },\n",
      "  {\n",
      "    \"text\": \"Sustainable ADA Signage \\u2013 Top Six 2019 Trends: https://t.co/j8RxOw34qI #Signage #ADA #Greenbuilding #Architecture\\u2026 https://t.co/jQqKgZLY2b\",\n",
      "    \"after_clean_text\": \"sustain signag top six ada http greenbuild trend\",\n",
      "    \"hypernyms\": [\n",
      "      \"continue\",\n",
      "      \"signag\",\n",
      "      \"region\",\n",
      "      \"digit\",\n",
      "      \"enzyme\",\n",
      "      \"protocol\",\n",
      "      \"greenbuild\",\n",
      "      \"direction\"\n",
      "    ],\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"text\": \"Nori's @CleanEnergyGrl is the third (!) Norinaut to be on @zengineeringpod! Y'all really can't get enough, huh? Che\\u2026 https://t.co/Hixb10A5ww\",\n",
      "    \"after_clean_text\": \"cant nori cleanenergygrl third huh yall enough norinaut get http zengineeringpod realli\",\n",
      "    \"hypernyms\": [\n",
      "      \"nonsense\",\n",
      "      \"nori\",\n",
      "      \"cleanenergygrl\",\n",
      "      \"common_fraction\",\n",
      "      \"huh\",\n",
      "      \"yall\",\n",
      "      \"relative_quantity\",\n",
      "      \"norinaut\",\n",
      "      \"return\",\n",
      "      \"protocol\",\n",
      "      \"zengineeringpod\",\n",
      "      \"realli\"\n",
      "    ],\n",
      "    \"score\": 0.075\n",
      "  },\n",
      "  {\n",
      "    \"text\": \"RT @alligroberts: Dustin Sarnoski from @StateStreet says that science-based targets are the goal that all companies should be working towar\\u2026\",\n",
      "    \"after_clean_text\": \"say dustin rt target goal work alligrobert compani sarnoski statestreet\",\n",
      "    \"hypernyms\": [\n",
      "      \"opportunity\",\n",
      "      \"dustin\",\n",
      "      \"rt\",\n",
      "      \"reference_point\",\n",
      "      \"content\",\n",
      "      \"activity\",\n",
      "      \"alligrobert\",\n",
      "      \"compani\",\n",
      "      \"sarnoski\",\n",
      "      \"statestreet\"\n",
      "    ],\n",
      "    \"score\": -0.15625\n",
      "  },\n",
      "  {\n",
      "    \"text\": \"RT @NIESRorg: Last few days to book on to our discussion on how to use #peer pressure to fight the #ClimateCrisis - We will be joined by @n\\u2026\",\n",
      "    \"after_clean_text\": \"peer day pressur fight book niesrorg use last join climatecrisi rt discuss\",\n",
      "    \"hypernyms\": [\n",
      "      \"person\",\n",
      "      \"time_unit\",\n",
      "      \"pressur\",\n",
      "      \"military_action\",\n",
      "      \"publication\",\n",
      "      \"niesrorg\",\n",
      "      \"activity\",\n",
      "      \"end\",\n",
      "      \"connection\",\n",
      "      \"climatecrisi\",\n",
      "      \"rt\",\n",
      "      \"field_event\"\n",
      "    ],\n",
      "    \"score\": 0.0\n",
      "  },\n",
      "  {\n",
      "    \"text\": \"RT @Princeton: Make sure not to miss @RutgersU professor \\u00c5sa Rennermalm\\u2019s fascinating lecture \\u201cWitnessing #ClimateChange: What I Have Learn\\u2026\",\n",
      "    \"after_clean_text\": \"lectur make fascin sure miss wit climatechang rutgersu professor rennermalm rt princeton \\u00e5sa\",\n",
      "    \"hypernyms\": [\n",
      "      \"lectur\",\n",
      "      \"kind\",\n",
      "      \"fascin\",\n",
      "      \"sure\",\n",
      "      \"woman\",\n",
      "      \"message\",\n",
      "      \"climatechang\",\n",
      "      \"rutgersu\",\n",
      "      \"academician\",\n",
      "      \"rennermalm\",\n",
      "      \"rt\",\n",
      "      \"princeton\",\n",
      "      \"\\u00e5sa\"\n",
      "    ],\n",
      "    \"score\": 0.041666666666666664\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# We load the url of the first file.\n",
    "url = \"Results\\\\\" + files_list[0] + \".json\"\n",
    "\n",
    "# We will open the file\n",
    "with open(url, 'r') as json_file:\n",
    "    data = json_file.read()\n",
    "\n",
    "# Now we parse the file\n",
    "parsed = json.loads(data)\n",
    "\n",
    "# Pretty Printing JSON string back\n",
    "print(json.dumps(parsed, indent = 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Run on all files.</h4>\n",
    "<p>Below we will run the implemented code on all the test files.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines = 6604 we're processed.\n",
      "Score assignation has finished successfully!\n"
     ]
    }
   ],
   "source": [
    "new_files_list = ['tweets_climatechange_06-03-2020', 'tweets_fridaysforfuture_09-03-2020', 'tweets_savetheplanet_09-03-2020']\n",
    "assign_scores(new_files_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
